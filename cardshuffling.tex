\documentclass[12pt]{article}

\input{../../../../etc/macros} %\input{../../../../etc/mzlatex_macros}
\input{../../../../etc/pdf_macros}

\bibliographystyle{plain}

\begin{document}

\myheader \mytitle

\hr

\sectiontitle{Card Shuffling as a Markov Chain}

\hr

\usefirefox

\hr

% \visual{Study Tip}{../../../../CommonInformation/Lessons/studytip.png}
% \section*{Study Tip}

% \hr

\visual{Rating}{../../../../CommonInformation/Lessons/rating.png}
\section*{Rating} %one of
% Everyone: contains no mathematics.
% Student: contains scenes of mild algebra or calculus that may require guidance.
Mathematically Mature:  may contain mathematics beyond calculus with
proofs. % Mathematicians Only: prolonged scenes of intense rigor.

\hr

\visual{Section Starter Question}{../../../../CommonInformation/Lessons/question_mark.png}
\section*{Section Starter Question}
Why shuffle a deck of cards?  What kind of shuffle do you use?  How
many shuffles are sufficient to achive the purpose of shuffling?

\hr

\visual{Key Concepts}{../../../../CommonInformation/Lessons/keyconcepts.png}
\section*{Key Concepts}
%% This section needs extensive rewriting.

\begin{enumerate}
    \item
        The identification of shuffles or operations with permutations
        themselves, \( Q \), the randomizing shuffle, is a probability
        distribution on \( S_n \).
    \item
        One realistic model of shuffling a deck of cards is the \defn{riffle
        shuffle}.
    \item
        The set of cuts and interleavings now induces in a natural way a
        density on the set of permutations.  Call this a \defn{riffle
        shuffle} and denote it by \( R \).  That is, \( R(\pi) \) is the
        sum of probabilities of each cut and interleaving that gives the
        rearrangement of the deck corresponding to \( \pi \).
    \item
        \( 7 \) shuffles the of 3-card deck gets very close to the
        uniform density, which turns out to be the stationary density.
    \item
        Hence the probability of any particular cut, followed by any
        particular interleaving is \( \frac{1}{2^n} \).
    \item
        The probability of achieving a permutation \( \pi \) when doing
        an \( a \)-shuffle is
        \[
            \frac{1}{a^n} \binom{n + a - r}{n},
        \] where \( r \) is the number of rising sequences in \( \pi \).
    \item
        \( 7 \) shuffles the of 3-card deck gets very close to the
        uniform density, which turns out to be the stationary density.
    \item
        the set of all eigenvalues of \( P \) is exactly \( 1 \), \(
        \frac{1}{2} \), \( \frac{1}{4} \) and \( \frac{1}{2^n} \).  The
        second largest eigenvalue determines the rate of convergence to
        the stationary distribution. For riffle shuffling, this
        eigenvalue is \( \frac{1}{2} \).
    \item
        A \defn{Top-to-Random Shuffle},%
        \index{top-to-random-shuffle}
        abbreviated TTRS hereafter, takes the top card from a stack of \(
        n \) cards and inserts it in the gap between the \( (k-1) \)th
        card and the \( k \)th card in the deck.
    \item
        For a finite, irreducible, aperiodic Markov chain \( Y_t \)
        distributed as \( Q^t \) at time \( t \) and with stationary
        distribution \( \pi \), and \( \tau \) is a strong stationary
        time, then
        \[
            \| Q^{\tau} - \pi \|_{TV} \le \Prob(\tau \ge t)
        \]
    \item
        Set \( d_n(t) = \| P^{\tau_{\text{top}}+1} - U \|_{TV} \).  Then
        for \( \epsilon > 0 \),
        \begin{enumerate}
            \item
                \( d_{n}(n \log n + n \log \epsilon^{-1} )\le \epsilon \)
                for \( n \) sufficiently large.
            \item
                \( d_{n}(n \log n - n \log (C \epsilon^{-1})) \ge 1-\epsilon
                \) for \( n \) sufficiently large.
        \end{enumerate}
\end{enumerate}

\hr

\visual{Vocabulary}{../../../../CommonInformation/Lessons/vocabulary.png}
\section*{Vocabulary}
\begin{enumerate}
    \item
        The \defn{riffle shuffle} first cuts the deck randomly into two
        packets, one containing \( k \) cards and the other containing \(
        n-k \) cards.  Choose \( k \), the number of cards cut according
        to the binomial density. Once the deck is cut into two packets,
        interleave the cards from each packet in any possible way, such
        that the cards from each packet keep their own relative order.
    \item
        A special case of this is the \defn{perfect shuffle}, also know
        as the \defn{faro shuffle} wherein the two packets are
        completely interleaved.
    \item
        A \defn{rising sequence} of a permutation is a maximal
        consecutive increasing subsequence.
    \item
        A \defn{\( a \)-shuffle} is another probability density on \( S_n
        \).  Let \( a \) be any positive integer.  Cut the deck into \(
        a \) packets of nonnegative sizes \( m_1, m_2, \dots, m_a \)
        with \( m_1 + \dots + m_a = n \) but some of the \( n_i \) may
        be zero. Interleave the cards from each packet in any way, so
        long as the cards from each packet, so long as the cards from
        each packet keep the relative order among themselves.  With a
        fixed packet structure, consider all interleavings equally
        likely.
    \item
        A defn{Top-to-Random Shuffle},%
        \index{top-to-random-shuffle}
        takes the top card from a stack of \( n \) cards and inserts it
        in the gap between the \( (k-1) \)th card and the \( k \)th card
        in the deck.
    \item
        The \defn{total variation distance} of \( \mu \) from \( \nu \)
        is%
        \index{total variation distance}
        \[
            \| \mu - \nu \|_{TV} = \max_{A \subset \Omega} \abs{ \mu(A)
            - \nu(A)} = \frac{1}{2} \sum\limits_{x \in \Omega} \abs{ \mu
            (x) - \nu(x)}.
        \]
    \item
        A \defn{strong stationary time}%
        for \( X_t \), \( t \ge 0 \) if \( X_{\tau_{\text{top}}+1} \sim
        \operatorname{unif}
        (S_n) \), and \( X_{\tau_{\text{top}}+1} \) is independent of \(
        \tau_{\text{top}} \).
\end{enumerate}

\hr

\visual{Mathematical Ideas}{../../../../CommonInformation/Lessons/mathematicalideas.png}
\section*{Mathematical Ideas}

\subsection*{General Setting}
% This section needs extensive revision to give clarity and precision.
% The explanation of transitions as composition of permuations need
% more explanation.

An unopened deck of cards has the face-up order (depending on manufacturer,
but typically in the U.S.), 
starting with the Ace of Spades:
\begin{itemize}
\item Ace, 2, 3, 4, 5, 6, 7, 8, 9, 10, Jack, Queen, King of Spades,
\item Ace, 2, 3, 4, 5, 6, 7, 8, 9, 10, Jack, Queen, King of Diamonds,
\item King, Queen, Jack, 10, 9, 8, 7, 6, 5, 4, 3, 2, Ace of Clubs,
  then
\item King, Queen, Jack, 10, 9, 8, 7, 6, 5, 4, 3, 2, Ace of Hearts.
\end{itemize}
Call this the initial order of the deck.  Knowing this order is
essential for some sleight of hand tricks performed by a
magician.  For card players, shuffling the deck to remove this order
is essential so that cards dealt from the deck come ``at random'',
that is, in an order uniformly distributed over all possible deck orders.
The main question here is: Starting from this order, how many shuffles are
necessary to obtain a ``random'' deck order from the uniform distribution?

In terms of Markov processes, the questions are: What is the state
space, what is an appropriate transition probability matrix, what is
the  steady state distribution, hopefully uniform, and how fast does
the Markov process approach the steady state distribution?

For simplicity and definiteness, let the cards in the initial deck order above be
numbered \( 1 \) to \( 52 \).  It will also be convenient to study much
smaller decks of cards, say having \( n \) cards.  The set of states for a Markov process
modeling the order of the deck is \( S_n \), the set of permutations on \( n \)
cards.  For convenience, set the initial state \( X_0
\) to be the identity permutation with probability \( 1 \).  In other
words, choose the initial distribution to reflect not having done
anything to the deck yet.

Consider some type of shuffle, that is, a re-ordering
operation on a state that takes the state to another state.  For
example, the riffle shuffle, also called a dovetail shuffle or leafing
the cards, is a common type of shuffle which
interleaves packets of cards.  A perfect riffle shuffle, also called a
faro shuffle, exactly splits
the deck in half, then interleaves exactly one card from each half.  A
perfect rifle shuffle difficult to
perform, except for practiced magicians.  More commonly, packets of
adjacent cards from  unevenly split portions interleave, creating a
new order for the deck which nevertheless preserves some of the
previous order in each packet.  Thus a particular riffle shuffle is one of a
whole family of riffle shuffles, chosen  with a probability
distribution on the family.  This probability distribution then
induces a transition probability from state to state, and thus a
Markov process.

Other types of shuffles have colorful names such as the Hindu shuffle,
pile shuffle, Corgi shuffle, Mongean shuffle, and Weave shuffle.
Some types of shuffle are a family of possible re-orderings
with probability distributions different from the riffle
shuffle, leading to different transition probabilities, and thus
different Markov processes.  The most well-studied type of shuffle is
the riffle shuffle and that is the focus here.

Going from card order\( \pi \) to \( \tau \) is
the same as composing \( \pi \) with the permutation \( \pi^{-1} \circ
\tau \).  Now identify shuffles as functions on  \( 1 \) to \( n \) to
\( 1 \) to \( n \),  that is permutations.
Since a particular riffle shuffle is one of a
whole family of riffle shuffles, chosen  with a probability
distribution \( Q \) from the family,
the transition probabilities are \( p_{\pi \tau} = \Prob{X_t =
\tau \given X_{t-1} = \pi} = Q(\pi^{-1} \circ \tau) \).  So now the
goal is to describe the probability distribution \( Q \) and apply it
to the Markov process.

% An immediate consequence of this is that the transition matrix for
% unshuffling is the transpose of the transition matrix since \( \hat{p}_{\pi
% \tau} = \hat{R}(\pi^{-1} \circ \tau) = R((\pi^{-1} \circ \tau)^{-1}) = R
% ((\tau^{-1} \circ \pi)) = p_{\tau \pi} \).

\subsection*{The Riffle Shuffle}
% Generally good explanation however
% The explanation of transitions as composition of permuations need
% more explanation.
% The formatting in the example needs improvement.

One realistic model of shuffling a deck cards is the \defn{riffle
shuffle}.%
\index{riffle shuffle}
The riffle shuffle is sometimes called the GSR shuffle since Gilbert and
Shannon and independently Reeds first analyzed it.  First cut the deck
randomly into two packets, one containing \( k \) cards and the other
containing \( n-k \) cards.  Choose \( k \), the number of cards cut
according to the binomial density, meaning that the probability of the
cut occurring after \( k \) cards is exactly \( \frac{1}{2^n}\binom{n}{k}
\).

Once the deck is cut into two packets, interleave the cards from each
packet in any possible way, such that the cards from each packet keep
their own relative order.  This means the cards originally in positions \(
1, 2, 3, \dots, k \) must still be in the same order after shuffling,
even if there are other cards in between.  The same goes for cards
originally in positions \( k+1, k+2, \dots, n \).  This requirement is
quite natural, considering how a person shuffles two packets of
cards, one in each hand.  The cards in the left hand must still be in
the same relative order in the shuffled deck, no matter how they
interleave with the cards in the other packet, because the cards drop in
order when shuffling.  The same goes for the cards in the right hand.
See Figure~%
\ref{fig:carshuffling:riffle} for an illustration of a riffle shuffle on
a \( 10 \)-card deck.

\begin{figure}
    \centering
\begin{asy}
  size(5inches);

real myfontsize = 12;
real mylineskip = 1.2*myfontsize;
pen mypen = fontsize(myfontsize, mylineskip);
defaultpen(mypen);

real eps = 0.1;
pair vert = (0, eps);
pair left = (-eps, 0);
pair right = (1.25, 0);

defaultpen(5);
path card = (0,0)--(1,0);

for(int i=0; i<6; ++i) {
  draw( shift(i * vert) * card);
}
for(int i=6; i<10; ++i) {
  draw( shift(left) * shift(i * vert) * card, red);
}

draw( shift(right) * shift( 0 * vert) * card );
draw( shift(right) * shift( 1 * vert) * card );  
draw( shift(right) * shift( 2 * vert) * card );  

draw( shift(left) * shift(right) * shift( 3 * vert) * card, red );  

draw( shift(right) * shift( 4 * vert) * card );  

draw( shift(left) * shift(right) * shift( 5 * vert) * card, red );  
draw( shift(left) * shift(right) * shift( 6 * vert) * card, red );  

draw( shift(right) * shift( 7 * vert) * card );  

draw( shift(left) * shift(right) * shift( 8 * vert) * card, red );  

draw( shift(right) * shift( 9 * vert) * card );  

int[] Pi = {2, 4, 5, 7, 1, 3, 6, 8, 9, 10};
for( int i=0; i<10; ++i) {
  label(string(10-i )+"$\qquad$"+string(Pi[9-i]), (2.75, eps * i));
}
label("$i\qquad\pi_i$", (2.75, eps * 10));
\end{asy}
    \caption{A riffle shuffle on a \( 10 \)-card deck cut into a top
    packet of \( 4 \) cards and bottom packaet of \( 6 \) cards.}%
    \label{fig:cardshuffling:riffle}
\end{figure}

A special case of this is the \defn{perfect shuffle},%
\index{perfect
  shuffle}
also known as the \defn{faro shuffle} wherein the two packets are
completely interleaved, one card from each hand following one card from
the other hand.  A perfect shuffle is easy to describe but difficult to
perform, except for practiced magicians.

Choose among all possible interleavings uniformly with \( k \) spots among \( n \)
places for the first packet, fixing the spots for the cards of the
other packet, giving \( \binom{n}{k} \) possible interleavings.  This
means the probability of any one interleaving has probability \( 1/\binom
{n}{k} \) of occurring. Hence the probability of any particular cut,
followed by any particular interleaving is \( \frac{1}{2^n}\binom{n}{k}
\cdot 1/\binom{n}{k} = \frac{1}{2^n} \).  Note that this probability has
no information about the cut or the interleaving.  The density on
possible  cuts and interleaving is uniform,.

The uniform density on the set of cuts and interleavings now induces in
a natural way a density on the set of permutations.  Call this a \emph{riffle
shuffle} and denote it by \( R \).  That is, \( R(\pi) \) is the sum of
probabilities of each cut and interleaving that gives the rearrangement
of the deck corresponding to \( \pi \).  In short, the chance of any
arrangement of cards occurring under riffle shuffling is the proportion
of cuts and interleavings that give that arrangement.

\begin{example}
    Consider the riffle shuffle on a \( 3 \)-card deck as a Markov
    chain. The probability distribution for \( R \) is in
    Table~\ref{tab:cardshuffling:riffle3}.  To obtain the entries in
    Table~\ref{tab:cardshuffling:riffle3}, 
    systematically go through the possible cuts and interleavings.
    Cutting three cards into the left packet, and none in the right packet, the only
    possible interleaving trivially leaves the deck unchanged.  With a
    cut into \( 2 \) cards on the left, \( 1 \) card on the right, one
    interleaving drops the right packet card on the bottom, the left
    packet cards as the top \( 2 \), leaving the deck unchanged.  Two
    other interleavings move the third card, in the right packet to
    the middle or the top.  The other two cuts are symmetric to the
    cuts described above, so \( 4 \) of the \( 8 \) cuts and
    interleavings keep the deck in the original order.  However, one
    shuffle each moves the formerly bottom card labeled \( 3 \) to the
    middle or top position, leaving cards \( 1 \) and \( 2 \) in that
    order in the shuffled deck.   With a riffle shuffle it is never
    possible to reverse the order of the deck.

    \begin{table}
      \centering
      \caption{Probability distribution for a riffle shuffle on a $3$
        card deck. }
    \begin{tabular}{ccccccc}
        $\pi$    & $[123]$       & $[213]$       & $[231]$       & $[132]$       & $[312]$       & $[321]$ \\ 
        $Q(\pi)$ & $\frac{1}{2}$ & $\frac{1}{8}$ & $\frac{1}{8}$ & $\frac{1}{8}$ & $\frac{1}{8}$ & 0.      \\ 
    \end{tabular}
      \label{tab:cardshuffling:riffle3}
    \end{table}

    To obtain the entries in Table~\ref{tab:cardshuffling:riffle3} do
    the computation for a typical element of the transition
    probability matrix, say \( p_{\pi,\tau} \) with \( \pi = [213] \)
    and \( \tau = [132] \). Then \( \pi^{-1} = [213] \) and \( \pi^{-1}
    \circ \tau = [231] \).  Now \( R([231]) = \frac{1}{8} \), giving \(
    p_{[213] [132]} = \frac{1}{8} \) in the probability transition
    matrix

    The full probability transition matrix under this ordering of the
    permutations is
    \[
        \bordermatrix{
            & [123]      & [213]      & [231]       & [132]       & [312]       & [321] \cr
       [123]& \frac{1}{2}& \frac{1}{8}& \frac{1}{8} & \frac{1}{8} & \frac{1}{8} & 0 \cr
       [213]& \frac{1}{8}& \frac{1}{2}& \frac{1}{8} & \frac{1}{8} & 0           & \frac{1}{8} \cr
       [231]& \frac{1}{8}& \frac{1}{8}& \frac{1}{2} & 0           & \frac{1}{8} & \frac{1}{8} \cr
       [132]& \frac{1}{8}& \frac{1}{8}& 0           & \frac{1}{2} & \frac{1}{8} & \frac{1}{8} \cr
       [312]& \frac{1}{8}& 0          & \frac{1}{8} & \frac{1}{8} & \frac{1}{2} & \frac{1}{8} \cr
       [321]& 0          & \frac{1}{8}& \frac{1}{8} & \frac{1}{8} & \frac{1}{8} & \frac{1}{2} \cr
        }
    \]

    Although in this case, the \( n=3 \) riffle shuffle, the matrix is
    symmetric, this is in general not true, the riffle shuffle with deck
    sizes greater than \( 3 \) is always nonsymmetric.  See the exercises.
\end{example}

First note that the Markov chain for riffle shuffling is regular, that
is, there is some number of shuffles after which any permutation has a
positive probability of appearing.  In fact, any number of shuffles
greater than \( \log_2 n \) will do.  Since the riffle shuffle Markov
chain is regular, there is a unique stationary density, which is the
uniform density on \( S_n \).

Now it is easy to verify with matrix multiplication that numerically,
speaking, starting with the identity ordering, the density of the
permutations after \( 7 \) riffle shuffles is the first row of \( P^7 \).
In fact,
\[
    P^7 =
    \begin{pmatrix}
        0.17059 & 0.16666       & 0.16666       & 0.16666       &
        0.16666 & 0.16278 \\
        0.16666 & 0.17059       & 0.16666       & 0.16666       &
        0.16278 & 0.16666 \\
        0.16666 & 0.16666       & 0.17059       & 0.16278       &
        0.16666 & 0.16666 \\
        0.16666 & 0.16666       & 0.16278       & 0.17059       &
        0.16666 & 0.16666 \\
        0.16666 & 0.16278       & 0.16666       & 0.16666       &
        0.17059 & 0.16666 \\
        0.16278 & 0.16666       & 0.16666       & 0.16666       &
        0.16666 & 0.17059 \\
    \end{pmatrix}
    .
\] That is, \( 7 \) shuffles the of 3-card deck gets very close to the
uniform density, which turns out to be the stationary density.

% start here
\subsection*{Probability of a Permutation Under Riffle Shuffle}

To find the density of \( k \) riffle shuffles \( R^{(k)} \), define a
\defn{rising sequence}%
\index{rising sequence}
of a permutation is a maximal consecutive increasing subsequence.  In
more detail, say \( x \) and look after it for the card labeled \( x+1 \).
If found, repeat the procedure and looking after the \( x+1 \) card for
the \( x+2 \) card.  Keep going in this manner until it is not possible
to find the next card.  Now go back to the original card \( x \) and
look for the \( x-1 \) card and so on.  When done, it is a rising
sequence.  A little thought shows that a deck breaks down as a disjoint
union of its rising sequences, since the union of any two consecutively
increasing subsequence that has that element is a rising subsequence.

\begin{example}
    Suppose that a permutation of a deck is \( 45162378 \).  Start with
    any card, say \( 3 \).  Look for \( 4 \) and do not find it.  Look
    before the \( 3 \) and find \( 2 \) and before it find \( 1 \).  So
    one of the rising sequences of this permutation is \( 123 \).  Now
    start again, this time with say \( 6 \).  Find \( 7 \) and then
    after it \( 8 \).  Before \( 6 \) find \( 5 \) and then \( 4 \).  So
    another rising sequence is \( 45678 \).  This accounts for all cards
    and the deck has only two rising sequences.  This is immediately
    clear when writing the sequence as \( 45_{1}6_{23}78 \), offsetting
    the two random sequences.
\end{example}

\begin{example}
    The riffle shuffle in Figure~%
    \ref{fig:cardshuffling:riffle} has two rising sequences, \( \pi(1) <
    \pi(2) < \pi(3) < \pi(4) \) and \( \pi(5) < \pi(6) < \pi(7) < \pi(8)
    < \pi(9) < \pi(10) \).
\end{example}

\begin{example}
    Note that \( 45_{1}6_{23}78 \) is a possible result of a riffle
    shuffle. Here the cut must divide the deck into two packets such
    that the length of each is the same as the length of the
    corresponding rising sequence.  So if the deck started in the
    natural order and the deck is cut into \( 123 \) and \( 45678 \),
    then the shuffle interleaves by taking \( 4 \), then \( 5 \), then \(
    1 \) then \( 6 \), then \( 2 \), then \( 7 \), then \( 8 \), thus
    obtaining the given order through riffling.
\end{example}

In general, a permutation \( \pi \) of \( n \) cards made by a riffle
shuffle will have exactly \( 2 \) rising sequences (unless it is the
identity which has exactly \( 1 \)).  Conversely any permutation of \( n
\) card with \( 1 \) or \( 2 \) rising sequences can be obtained by a
physical shuffle. Therefore a mathematical definition of a riffle
shuffle can be made as ``a permutation with \( 1 \) or \( 2 \) rising
sequences.'' Suppose \( c \) cards are cut off the top.  Then there are \(
\binom{n}{c} \) possible riffle shuffles, (one of which is the identity
shuffle).  The reason is easy to see from Figure~%
\ref{fig:cardshuffling:riffle}, after the shuffle, the red and black
cards form a binary \( n \)-tuple with \( c \) red cards, there are \(
\binom{n}{c} \) such \( n \)-tuples.  The total number of possible
riffle shuffles is
\[
    1 + \sum\limits_{c=0}^n \left( \binom{n}{c} - 1\right) = 2^n - n.
\]

The next goal is to get similar results about what happens after
multiple riffle shuffles.  This can be done by considering \( a \)-shuffles.
A \defn{\( a \)-shuffle} is another probability density on \( S_n \).
Let \( a \) be any positive integer.  Cut the deck into \( a \) packets
of nonnegative sizes \( m_1, m_2, \dots, m_a \) with \( m_1 + \dots + m_a
= n \) but some of the \( n_i \) may be zero.  The probability of this
particular packet structure is given by the multinomial density:
\[
    \frac{1}{a^n} \binom{n}{m_1, m_2, \dots, m_a}.
\] Interleave the cards from each packet in any way, so long as the
cards from each packet, so long as the cards from each packet keep the
relative order among themselves.  With a fixed packet structure,
consider all interleavings equally likely.  Count the number of such
interleavings as the number of ways of choosing among \( n \) positions
in the deck, \( m_1 \) places for things of one type, \( m_2 \) places
for things of another type and so on.  The count is the multinomial
coefficient
\[
    \binom{n}{m_1, m_2, \dots, m_a}.
\] Hence the probability of a particular rearrangement, i.e.\ a cut of
the deck and an interleaving is
\[
    \frac{1}{a^n} \binom{n}{m_1, m_2, \dots, m_a} \cdot \binom{n}{m_1, m_2,
    \dots, m_a} = \frac{1}{a^n}.
\] So it turns out that each combination of a parituclar cut into
packets and an interleaving is equally likely, just as in the riffle
shuffle.  The induced density on the permutations leading to the cuts
and shuffles is then called the \( a \)-shuffle,%
\index{$a$-shuffle}
with notation \( R_a \).  The riffle shuffle is just the \( 2 \)-shuffle,
so \( R_2 = R \).

An equivalent description of the \( a \)-shuffle begins the same way, by
cutting the deck into packets multinomially.  Then drop cards from the
bottom of the packets, one at a time, such that the probability of
choosing a particular packet to drop is proportional to the relative
size of that packet compared to the number of all cards in the packets.
The proof of this description is exactly analogous to the \( a = 2 \)
case.

A third equivalent description is cutting the deck multinomially into
packets of size \( m_1, m_2, \dots m_n \) and riffling \( m_1 \) and \(
m_2 \) together, meaning choose uniformly among all interleaving that
the keep the relative order of each packet, then riffling the resulting
pile with \( m_3 \), then riffling that resulting pile with \( m_4 \)
and so on.

It turns out that when performing a single \( a \)-shuffle, the
probability of achieving a particular permutation \( \pi \) does not
depend on the information contained in \( \pi \), but only on the number
of rising sequences that \( \pi \) has.  In other words, the
permutations \( [12534], [34512], [51234] \), and \( [23451] \) all have
the same probability under any \( a \)-shuffle, since they all have
exactly two rising sequences.

A useful code through \( n \)-digit base-\( a \) numbers specifies how
to make a particular \( a \)-shuffle.  Here a ``shuffle'' indicates a
particular way of rearranging the deck, not the probability density of
all such rearrangements.  Let \( A \) be an \( n \)-digit base-\( a \)
number. Count the number of \( 0 \)s in \( A \), this will be the size
of the first packet \( m_1 \) in the \( a \)-shuffle.  Then \( m_2 \) is
the number of \( 1 \)s in \( A \) and so on up to \( m_a \), the number
of \( (a-1) \)s.  This cuts the deck into \( a \) packets.  Not take the
beginning packet of cards of size \( m_1 \).  Envision placing these
cards on top of all the \( 0 \) digits keeping their order as a rising
sequence.  DO the same for the next packet of size \( m_2 \), placing
them on the \( 1 \)s.  Continue up through the \( (a-1) \)s,.  This
particular way of rearranging the cards will then be the particualr cut
and the interleaving corresponding to \( A \).  Note that the number of
such encodings is \( a^n \).

\begin{example}
    Let the deck start in natural order.  Let \( A = 23004103 \) be the
    code for a particular \( 5 \)-shuffle of the \( 8 \)-card deck.  The
    are three \( 0 \)s, one \( 1 \), one \( 2 \), two \( 3 \)s and one \(
    4 \) Thus \( p_1 = 3 \), \( p_2 = 1 \), \( p_3 =1 \), \( p_4 = 2 \)
    and \( p_5 = 1 \).  So the cut deck into \( 123|4|5|67|8 \).  We put
    \( 123 \) where the \( 0 \)s are in \( A \), \( 4 \) where the \( 1 \)
    is, \( 5 \) where the \( 2 \) is, \( 67 \) where the \( 3 \)s are,
    and \( 8 \) where the \( 4 \) is.  Then get a shuffled deck of \(
    56128437 \) after applying \( A \) to the natural order.
\end{example}

This code gives a bijective correspondence between \( n \)-digit base-\(
a \) numbers and the set of all ways of cutting and interleaving an \( n
\)-card deck according to the \( a \)-shuffle.  In fact, if we put the
uniform denstiy oon the set of \( n \)-digit base-\( a \) numbers, this
transfers to the correct uniform probability density for cutting and
interleaving in an \( a \)-shuffle which means the correct density in
induced on \( S_n \).

\begin{theorem}
    The probability of achieving a permutation \( \pi \) when doing an \(
    a \)-shuffle is
    \[
        \frac{1}{a^n} \binom{n + a - r}{n},
    \] where \( r \) is the number of rising sequences in \( \pi \).
\end{theorem}

\begin{proof}
    \begin{enumerate}
        \item
            If we establish and fix where the \( (a-1) \) cuts occur in
            an \( a \)-shuffle, then whatever permutations can actually
            be achieved by interleaving the cards from this cut/packet
            structure can be achieved in exactly one way:  Just drop
            thte cards in exactly the order of the permutation.
        \item
            Thus the probability of achieving a particular permutation
            is the number of possible ways of making cuts that could
            actually cause that permutation, divided by the total number
            of ways of making cuts and interleaving for an \( a \)-shuffle.
        \item
            If we have \( r \) rising sequences in \( \pi \), we know
            exactly where \( (r-1) \) of the cuts have to have been:
            between pairs of consecutive cards in the narually ordered
            deck such that the first card ends one rising sequence of \(
            \pi \).
        \item
            This means that we have \( (a-1) - (r-1) = a-r \)
            unspecified or free cuts which can go anywhere.
        \item
            So count the number of ways of putting \( a-r \) cuts among \(
            n \) cards.  The standard ``stars and bars'' combinatorial
            argument counts
            \[
                \binom{n + a-r}{n}
            \] ways to do this, i.e.\ choosing \( n \) places among \( (a-r)
            + n \).
        \item
            This is the numerator of the probability expressed as a
            fraction.  The denominator is the number of possible ways to
            cut and interleave for an \( a \)-shuffle.  In encoding the
            number of shuffles, then are \( a^n \) ways to do this, the
            number of \( n \)-digit base \( a \) numbers.
    \end{enumerate}
\end{proof}

\subsection*{Rate of convergence to the stationary distribution}

The rate of convergence of the \( X_t \) to the stationary density,
measured by the total variation distance of soem other metric is
determined by the eigenvalues of hte transition matrix.  We know that
the entries of \( P^k \) are the probabilities of certain permutations
being acheived usder \( k \) riffle shuffles.  These probabilities are
of the form
\[
    \frac{1}{2^nk} \binom{2^k +n - r}{n},
\] for the probability of a permutation with \( r \) rising sequences
being achieved after \( k \) riffle shuffles.  Expanding the binomial
expression, explicitly write out
\[
    \binom{x + n - r}{n} = \sum\limits_{i=0}^n c_{n,r,i} x^i
\] giving the binomial coefficient as an \( n \)th degree polynomial in \(
x \) with coefficients as a function of \( n \) and \( r \).  It doesn't
really matter what the coefficients are, only that we can write the
binomial coefficient expansion as a polynomial in \( x \).  Substituting
\( 2^k \) for \( x \), we see tha thte entries of \( P^k \) have the
form
\[
    \frac{1}{2^{nk}} \sum\limits_{i=0}^n c_{n,r,i} \left( 2^k \right)^i
    = \sum\limits_{i=0}^n c_{n,r,n-i} \left( \frac{1}{2^i} \right)^k
\] This means that the entries of the \( k \)th power of \( P \) are
given by fixed linear combinations of hte \( k \)th powers of \( 1 \), \(
\frac{1}{2} \), \( \frac{1}{4} \) and \( \frac{1}{2^n} \).  Then from
standard facts of linear algebra the set of all eignevalues of \( P \)
is exactly \( 1 \), \( \frac{1}{2} \), \( \frac{1}{4} \) and \( \frac{1}
{2^n} \).  The second largets eigenvalue determines the rate of
convergence to the stationary distribution. For riffle shuffling, this
eigenvalue is \( \frac{1}{2} \).  Once the variation distance gets to
the cutoff, it decreases by a facotr of approximately \( \frac{1}{2} \)
with each shuffle.

\begin{example}
    The riffle shuffle matrix for the deck of three cards is
\[
    \begin{pmatrix}
        \frac{1}{2}     & \frac{1}{8}   & \frac{1}{8}   & \frac{1}{8}
        & \frac{1}{8}   & 0 \\
        \frac{1}{8}     & \frac{1}{2}   & \frac{1}{8}   & \frac{1}{8}
        & 0     & \frac{1}{8} \\
        \frac{1}{8}     & \frac{1}{8}   & \frac{1}{2}   & 0     & \frac{1}
        {8}     & \frac{1}{8} \\
        \frac{1}{8}     & \frac{1}{8}   & 0     & \frac{1}{2}   &
        \frac{1}{8}   & \frac{1}{8} \\
        \frac{1}{8}     & 0     & \frac{1}{8}   & \frac{1}{8}   & \frac{1}
        {2}     & \frac{1}{8} \\
        0       & \frac{1}{8}   & \frac{1}{8}   & \frac{1}{8}   & \frac{1}
        {8}     & \frac{1}{2} \\
    \end{pmatrix}
\]
    The eigenvalues of this matrix are \( 1, \frac{1}{2}, \frac{1}{2},
    \frac{1}{2}, \frac{1}{4}, \frac{1}{4} \).
\end{example}

\subsection*{Top to Random Shuffle}

A defn{Top-to-Random Shuffle},%
\index{top-to-random-shuffle}
abbreviated TTRS hereafter, takes the top card from a stack of \( n \)
cards and inserts it in the gap between the \( (k-1) \)th card and the \(
k \)th card in the deck.  See Figure~%
\ref{fig:cardshuffling:cards1}. (Note that \( k = 1 \) is possible, in
which case the top card returns to the top.  Likewise, \( k = n+1 \) is
also permitted, in which case the top card moves to the bottom of the
card stack.)

Consider the order of the cards to be a permutation on an alphabet of \(
n \) symbols.  Then after \( t \) TTRS shuffles, the order of the deck
has a probability distribution \( P^t_n = P^t \) on \( S_n \) where the
subscript \( n \) is omitted for brevity for a fixed or implicit number
of cards.

The TTRS is naturally a finite Markov chain \( X_t \) for \( t \ge 0 \)
with \( X_t \in S_n \).  The transition probabilities are
\[
    \Prob{X_{t+1} = \sigma' \given X_t = \sigma} =
    \begin{cases}
        \frac{1}{n}     & \text{\( \sigma' \) is a TTRS of \( \sigma \)}\\
        0       & \text{otherwise}
    \end{cases}
\] Additionally, it is natural to set \( X_0 = \sigma_0 \), the identity
permutation.  Then \( X_t \sim P^t \), the distribution on card decks.
It is routine to check to that \( X_t \) is irreducible.  It is also
immediate that \( X_t \) is aperiodic since it is possible that the top
card can be place back on top.  Therefore, this Markov chain must
converge to a stationary distribution and by the previous Theorem, \( P^t
\to
\operatorname{unif}
(S_n) \).

The transition matrix for the TTRS on a deck with three cards is
\[
    \bordermatrix{
        & [123] & [213] & [231] & [132] & [312] & [321] \cr
        [123]   & \frac{1}{3}   & \frac{1}{3}   & \frac{1}{3}   & 0
        & 0     & 0 \cr
        [213]   & \frac{1}{3}   & \frac{1}{3}   & 0     & \frac{1}{3}
        & 0     & 0 \cr
        [231]   & 0     & 0     & \frac{1}{3}   & 0     & \frac{1}{3}
        & \frac{1}{3} \cr
        [132]   & 0     & 0     & 0     & \frac{1}{3}   & \frac{1}{3}
        & \frac{1}{3} \cr
        [312]   & \frac{1}{3}   & 0     & 0     & \frac{1}{3}   & \frac{1}
        {3}     & 0 \cr
        [321]   & 0     & \frac{1}{3}   & \frac{1}{3}   & 0     & 0
        & \frac{1}{3} \cr
    }
\]

If the card deck is initially in order \( 1 \) to \( n \) from top to
bottom, how many of these shuffles does it take for the deck to be
sufficiently shuffled? It is easy to verify with matrix multiplication
that numerically, speaking, starting with the identity ordering, the
density of the permutations after \( 7 \) top-to-random shuffles is the
first row of \( P^7 \).  In fact,
\[
    P^7 =
    \begin{pmatrix}
      0.16690 & 0.16690 & 0.16690 & 0.16644 & 0.16644 & 0.16644 \\
      0.16690 & 0.16690 & 0.16644 & 0.16690 & 0.16644 & 0.16644 \\
      0.16644 & 0.16644 & 0.16690 & 0.16644 & 0.16690 & 0.16690 \\
      0.16644 & 0.16644 & 0.16644 & 0.16690 & 0.16690 & 0.16690 \\
      0.16690 & 0.16644 & 0.16644 & 0.16690 & 0.16690 & 0.16644 \\
      0.16644 & 0.16690 & 0.16690 & 0.16644 & 0.16644 & 0.16690 \\
    \end{pmatrix}
    .
\] That is, \( 7 \) shuffles of the 3-card deck gets close to the
uniform density, which turns out to be the stationary denisty. Part of
this question is to refine what it means for the deck to be sufficiently
shuffled.

The eigenvalues of this matrix are \( 1, \frac{1}{3}, \frac{1}{3}, \frac
{1}{3}, 0, 0 \).

\begin{figure}
    \centering
\begin{asy}
  size(5inches);

real myfontsize = 12;
real mylineskip = 1.2*myfontsize;
pen mypen = fontsize(myfontsize, mylineskip);
defaultpen(mypen);

real eps = 0.1;
pair vert = (0, eps);

defaultpen(5);
path card = (0,0)--(1,0);

label("Stack Position", shift(3*vert)*(1.1,0));
draw(shift(2*vert)*card); label("$1$", shift(2*vert)*(1.1,0));
draw(shift(vert)*card); label("$2$", shift(vert)*(1.1,0));
draw(card); label("$3$",(1.1,0));
label("$\Huge{\vdots}$", -vert);
draw(shift(-2vert)*card); label("$k-1$", shift(-2*vert)*(1.1,0));
draw(shift(-3*vert)*card); label("$k$", shift(-3*vert)*(1.1,0));
label("$\Huge{\vdots}$", -4*vert);
draw(shift(-5*vert)*card); label("$n$", shift(-5*vert)*(1.1,0));

draw( arc( (1.15, -eps/4), r = 2.20*eps, angle1=90, angle2=-90),
      arrow=Arrow(), red+1bp);
\end{asy}
    \caption{Schematic drawing of the Top-to-Random-Shuffle.}%
    \label{fig:cardshuffling:cards1}
\end{figure}

\begin{lemma}
    At any time \( t \), if there are \( k \) cards beneath the card
    labeled \( n \), then these cards appear in any order with equal
    probability.
\end{lemma}

\begin{figure}
    \centering
\begin{asy}
    size(5inches);

real myfontsize = 12;
real mylineskip = 1.2*myfontsize;
pen mypen = fontsize(myfontsize, mylineskip);
defaultpen(mypen);

real eps = 0.1;
pair vert = (0, eps);

defaultpen(2);
path card = (0,0)--(1,0);

picture p = new picture;
size(p, 2inches);

label(p, "Card Number", shift(3*vert)*(-0.1,0));
draw(p, shift(2*vert)*card);
draw(p, shift(vert)*card);
draw(p, card);
label(p, "$\Large{\vdots}$", -vert);
label(p, "$n$", shift(-2*vert)*(-0.1,0));
draw(p, shift(-2*vert)*card); 
draw(p, shift(-3*vert)*card); 
label(p, "$\Large{\vdots}$", -4*vert);
draw(p, shift(-5*vert)*card);

draw(p, (1.15, -2*eps)--(1.15, -5*eps),
     arrow=Arrows(),
     bar= Bars(), black+1bp );
label(p, "$k$ cards", (1.15, -3.5*eps), align=E );
draw(p,  arc( (1.15, 0), r = eps, angle1=90, angle2=-90),
      arrow=Arrow(), red+1bp);

picture q = new picture;
size(q, 2inches);

label(q, "Card Number", shift(3*vert)*(-0.1,0));
draw(q, shift(2*vert)*card);
draw(q, shift(vert)*card);
draw(q, card);
label(q, "$\Large{\vdots}$", -vert);
label(q, "$n$", shift(-2*vert)*(-0.1,0));
draw(q, shift(-2*vert)*card); 
draw(q, shift(-3*vert)*card); 
label(q, "$\Large{\vdots}$", -4*vert);
draw(q, shift(-5*vert)*card);

draw(q, (1.15, -2*eps)--(1.15, -5*eps),
     arrow=Arrows(),
     bar= Bars(), black+1bp );
label(q, "$k$ cards", (1.15, -3.5*eps), align=E );
draw(q,  arc( (1.15, -eps), r = 3*eps, angle1=90, angle2=-90),
      arrow=Arrow(), red+1bp);

add(p.fit(),(0,0), (0,0) );
add(q.fit(),(0,0), (100,0) );
\end{asy}
    \caption{Schematic diagram of the proof of the Lemma}%
    \label{fig:cardshuffling:cards2}
\end{figure}
\begin{proof}
    The proof is by induction on \( t \).  The base case \( t = 0 \) is
    trivial.  Suppose that the claim is true for some \( t > 0 \).  In
    the transition to \( t + 1 \), two cases can occur, refer to Figure~%
    \ref{fig:cardshuffling:cards2} for a schematic diagram. First, the
    top card is randomly placed above the card labeled \( n \) that is
    somewhere in the stack.  Then nothing is changed and the proof is
    complete.  Otherwise, the top card is placed in one of the \( k+1 \)
    available spaces below the card labled \( n \) that is somewhere in
    the stack.  The probability of any particular one of these
    arrangements is
    \[
        \frac{1}{k!} \cdot \frac{1}{k+1} = \frac{1}{(k+1)!}
    \] where \( \frac{1}{k!} \) comes from the induction hypothesis and
    the \( \frac{1}{k+1} \) comes from the TTRS\@.  The proof is
    complete.
\end{proof}

\begin{theorem}
    Let \( \tau_{\text{top}} \) be the first time that card \( n \)
    reaches the top of the deck.  Then \( P^{\tau_{\text{top}}+1} \) is
    uniform on \( S_n \).  Furthermore, whatever permutation arises at
    time \( \tau_{\text{top}}+1 \) is independent of \( \tau_{\text{top}}
    \).
\end{theorem}

\begin{proof}
    Follows easily from the Lemma, since at time \( \tau_{\text{top}} \)
    the \( n-1 \) cards below card \( n \) will be uniformly distributed
    over the \( (n-1)! \) possible permutations.  Then at time \( \tau_{\text
    {top}}+ 1 \) card \( n \) is inserted uniformly at random in the
    deck.
\end{proof}

\begin{remark}
    Waiting for \( \tau_{\text{top}} \) is the same as waiting for
    completion in the ``coupon collectors problem in reverse''.  More
    precisely, collecting a coupon here is like putting the card below
    the card labeled \( n \).  The first card is hard to put under \( n \),
    in fact it happens with probability \( \frac{1}{n+1} \) but it gets
    easier as time goes on.  This motivates the assertion that \( \E{\tau_
    {\text{top}} + 1} = \Theta(n \log n) \) and that \( \Prob{\tau_{\text
    {top}}+1 \ge n \log n + c n} \le \EulerE^{-c} \) for all \( c \ge 0 \).
\end{remark}

\begin{definition}
    If \( \mu \) and \( \nu \) are probability distributions on \(
    \Omega \), the \defn{total variation distance} of \( \mu \) from \(
    \nu \) is%
    \index{total variation distance}
    \[
        \| \mu - \nu \|_{TV} = \max_{A \subset \Omega} \abs{ \mu(A) -
        \nu(A)} = \frac{1}{2} \sum\limits_{x \in \Omega} \abs{ \mu(x) -
        \nu(x)}.
    \]
\end{definition}

\begin{remark}
    A ``bad event'' is an event with \( \mu \) and \( \nu \) measure \(
    A \) very differently.
\end{remark}

\begin{definition}
    We say that \( \tau_{\text{top}} \) is a \defn{strong stationary
    time}%
    \index{strong stationary time}
    for \( X_t \), \( t \ge 0 \) if \( X_{\tau_{\text{top}}+1} \sim
    \operatorname{unif}
    (S_n) \), and \( X_{\tau_{\text{top}}+1} \) is independent of \(
    \tau_{\text{top}} \).
\end{definition}

\begin{theorem}[Aldous, Diaconis]
    For a finite, irreducible, aperiodic Markov chain \( Y_t \)
    distributed as \( Q^t \) at time \( t \) and with stationary
    distribution \( \pi \), and \( \tau \) is a strong stationary time,
    then
    \[
        \| Q^{\tau} - \pi \|_{TV} \le \Prob(\tau \ge t)
    \]
\end{theorem}

\begin{remark}
    The hypotheses irreducible and aperiodic may not be strictly
    necessary, but occur here because they are common in theorems about
    Markov chains.
\end{remark}

Then immediately, \( \| P^{\tau_{\text{top}}+1} - U \|_{TV} \le \Prob{\tau_
{\text{top}+1} \le \EulerE^{-c}} \).  This is like the coupon collector
having \( n \) coupons.

For simplicity in what follows, set \( d_n(t) = \| P^{\tau_{\text{top}}+1}
- U \|_{TV} \).  Then for \( \epsilon > 0 \),
\begin{enumerate}
    \item
        \( d_{n}(n \log n + n \log \epsilon^{-1} )\le \epsilon \) for \(
        n \) sufficiently large.
    \item
        \( d_{n}(n \log n - n \log (C \epsilon^{-1})) \ge 1-\epsilon \)
        for \( n \) sufficiently large.
\end{enumerate}

The justification is to find a ``bad event'' and use it to measure the
total variation distance.  In fact, let \( A_j \) be the event that the
bottom \( j \) cards of the deck appear in correct relative order.  Then
\( U(A_j) = 1/j! \).  while \( P^t(A_j) = \).

This gives a cut-off phenomenon, see Figure~%
\ref{fig:cardshuffling:cards3}

\begin{figure}
    \centering
\begin{asy}
    import graph;

size(5inches);

real myfontsize = 12;
real mylineskip = 1.2*myfontsize;
pen mypen = fontsize(myfontsize, mylineskip);
defaultpen(mypen);

real f( real x) {
  real a = 0.6;
  real k = 50.0;
  
  real term = exp(-k*(x -a));
  return term/( 1 + term);
}

draw( graph(f, 0,1));
xaxis("$t$", Arrow);
xtick(Label("$n \log n (1 -o(1))$", (0.4,0), 2S), (0.4, 0), S);
xtick(Label("$n \log n $", (0.6, 0), 2N), (0.6, 0), N);
xtick(Label("$n \log n (1 +o(1))$", (0.8,0), 2S), (0.8, 0), S);
yaxis("$\| P^t - U \|_{TV}$", Arrow);
\end{asy}
    \caption{Schematic graph of the cut-off phenomenon for the Total
    Variation distance of the Markov chain distribution from the uniform
    distribution as a function of the number of steps.}%
    \label{fig:cardshuffling:cards3}
\end{figure}
\begin{remark}
    Note that \( n \log n + n \log \epsilon^{-1} = n \log n (1 + o(1)) \)
    and\( n \log n - n \log \epsilon^{-1} = n \log n (1 - o(1)) \)
\end{remark}

\visual{Section Starter Question}{../../../../CommonInformation/Lessons/question_mark.png}
\section*{Section Ending Answer}

\subsection*{Sources} This section is adapted from
\cite{mann94}.  Parts are also adapted from
\cite{aldous86}.

\hr

\visual{Algorithms, Scripts, Simulations}{../../../../CommonInformation/Lessons/computer.png}
\section*{Algorithms, Scripts, Simulations}

\subsection*{Algorithm}

\subsection*{Scripts}

% \input{ _scripts}

\hr

\visual{Problems to Work}{../../../../CommonInformation/Lessons/solveproblems.png}
\section*{Problems to Work for Understanding}
% Reformat the Problem and Solutions to exsol format.
\begin{exercise}
  Show with a specific pair of permutations on 4 cards that the
  transition probability matrix is not symmetric.
\end{exercise}
\begin{solution}
  
\end{solution}

% \link{_soln.xml}{Solutions to Problems}

\hr

\visual{Books}{../../../../CommonInformation/Lessons/books.png}
\section*{Reading Suggestion:}

\bibliography{../../../../CommonInformation/bibliography}

%   \begin{enumerate}
%     \item
%     \item
%     \item
%   \end{enumerate}

\hr

\visual{Links}{../../../../CommonInformation/Lessons/chainlink.png}
\section*{Outside Readings and Links:}
\begin{enumerate}
    \item
    \item
    \item
    \item
\end{enumerate}

\hr

\mydisclaim \myfooter

Last modified:  \flastmod

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
